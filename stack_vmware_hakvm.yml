---
name: vmware_kvm

# Determine IP of local host
host_ip: &host_ip "#{require 'socket'; UDPSocket.open {|s| s.connect('8.8.8.8', 1); s.addr.last}}"

environment: vagrant_hakvm

chef:
  knife_config:
    ssl_verify_mode: verify_none

vagrant:
  provider: vmware_fusion
  box_name: mevansam/chef-ubuntu-14.04
  box_url: https://vagrantcloud.com/mevansam/boxes/chef-ubuntu-14.04

stack:


#######################
# HAProxy Load Balancer

# The HAproxy load balancer that frontends all
# the OpenStack services. When services are
# scaled out the loadbalancer will discover the
# added/deleted nodes via Chef Ohai.
- node: openstack-proxy
  scale: 2
  attributes:
    cluster_name: proxy
    pacemaker_mcast_address: 239.255.42.1
    pacemaker_mcast_port: 5405
  knife: <<+[./common/vagrant.yml][vagrant_ubuntu_1404][create_2nic_768M]

# This node combines all the HAproxy instances
# into a pacemaker cluster. It does not create
# any VMs but it will replace the run list of
# the target VMs with the one given.
- node: openstack-proxy-cluster
  depends_on:
  - compute-services
  targets:
  - openstack-proxy
  attributes:
    haproxy: <<+env[proxy]
  run_list:
  - recipe[ohai]
  - recipe[ntp]
  - recipe[network]
  - role[os-ha-proxy]


############################
# OpenStack compute services

# Node cluster that will host all compute services
- node: compute-services
  scale: 1
  depends_on:
  attributes:
    cluster_name: services
  knife: <<+[./common/vagrant.yml][vagrant_ubuntu_1404][create_1nic_4G]

# This node install all the OpenStack services
# including the database and queue on the compute
# services cluster nodes
- node: compute-services-cluster
  depends_on:
  - openstack-proxy-cluster
  targets:
  - compute-services
  attributes:
    # Override optimized settings in os-ha-database
    # so mysql will run in a much smaller environment
    percona:
      haproxy_cluster_name: proxy
      server:
        key_buffer: 16M,
        max_allowed_packet: 64M,
        thread_stack: 192K,
        thread_cache_size: 16,
        query_cache_size: 64M,
        query_cache_limit: 2M,
        innodb_buffer_pool_size: 128M
    openstack:
      logging:
        syslog_endpoint:
          protocol: udp
          hosts:
          - *host_ip
      endpoints:
        db:
          host: "#{env['openstack']['endpoints']['openstack_ops_proxy']}"
        mq:
          host: "#{env['openstack']['endpoints']['openstack_ops_proxy']}"
      block-storage:
        volume:
          create_volume_group: true
          create_volume_group_type: block_devices
          block_devices: /dev/sdb
  run_list:
  - recipe[ohai]
  - recipe[ntp]
  - recipe[network]
  - role[os-ha-database]
  - role[os-ha-messaging]
  - role[os-ha-dashboard]
  - role[os-ha-identity]
  - role[os-ha-image]
  - role[os-ha-block-storage-kvm-lvm]
  - role[os-block-storage-volume]
  - role[os-ha-controller-kvm]
  run_on_event:
    configure: <<+[./common/patches.yml][patch_neutron_nova_ssl_notification]


########################
# OpenStack compute host

# Hypervisor node
- node: compute-host
  scale: 2
  attributes:
    # Cluster every 2 instances. i.e. instance 0 and 1
    # will be put into a cluster named 'network-node-0'
    # and 2 and 3 into a cluster named 'network-node-1'
    # and so on.
    cluster_name: <<!'network-node-'+(#{index}/2).to_s
    pacemaker_mcast_address: 239.255.42.2
    pacemaker_mcast_port: 5405
  knife: <<+[./common/vagrant.yml][vagrant_ubuntu_1404][create_2nic_1G]

# Sets up the nova compute and neutron network
# cluster. The clusters are named such that every
# 2 consecutive nodes are named same. Each of these
# two nodes will be combined into a Pacemaker cluster
# that will run the Neutron L3 agents.
- node: compute-network-cluster
  depends_on:
  - compute-services-cluster
  targets:
  - compute-host
  attributes:
    env:
      network_interfaces:
      - device: eth2
    openstack:
      logging:
        syslog_endpoint:
          protocol: udp
          hosts:
          - *host_ip
      endpoints:
        db:
          host: "#{env['openstack']['endpoints']['openstack_ops_proxy']}"
        mq:
          host: "#{env['openstack']['endpoints']['openstack_ops_proxy']}"
      network:
        l3:
          is_clustered: true
  run_list:
  - recipe[ohai]
  - recipe[ntp]
  - recipe[network]
  - role[os-ha-compute-kvm]
  - role[os-ha-network-kvm]
  run_on_event:
    configure: <<+[./common/patches.yml][patch_nova_compute_libvirt_permissions]
